{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data\n",
    "> All of the data processing code I use for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroSEED embeddings - hyperbolic/Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first: gzip all the embeddings so we don't run into LFS limits\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "embed_dir = \"../data/otu_embeddings/greengenes\"\n",
    "for file in os.listdir(embed_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        old_path = os.path.join(embed_dir, file)\n",
    "        new_path = os.path.join(\n",
    "            embed_dir,\n",
    "            file.replace(\"embeddings_\", \"\").replace(\"hyperbolic\", \"H\").replace(\"euclidean\", \"E\").replace(\"_\", \"\")\n",
    "            + \".gz\",\n",
    "        )  # \"embeddings_\" prefix redundant\n",
    "        print(f\"Compressing {file}\")\n",
    "        pd.read_csv(old_path).to_csv(new_path, compression=\"gzip\")\n",
    "        print(f\"Compressed {file} from {os.path.getsize(old_path)} to {os.path.getsize(new_path)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! Now we can delete the old embeddings to clear some space\n",
    "for file in os.listdir(embed_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(embed_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added these datasets to huggingface:\n",
    "https://huggingface.co/datasets/pchlenski/greengenes_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRepo embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 10037 × 27105\n",
      "    obs: 'ravel_black-hispanic', 'ravel_nugent-category', 'ravel_nugent-score', 'ravel_ph', 'ravel_white-black', 'dataset', 'gevers_ileum', 'gevers_pcdai-ileum', 'gevers_pcdai-rectum', 'gevers_rectum', 'sokol_healthy-cd', 'sokol_healthy-uc', 'yatsunenko_baby-age', 'yatsunenko_malawi-venezuela', 'yatsunenko_sex', 'yatsunenko_usa-malawi', 'hmp_gastro-oral', 'hmp_sex', 'hmp_stool-tongue-paired', 'hmp_sub-supragingivalplaque-paired', 'cho_control-ct-cecal', 'cho_control-ct-fecal', 'cho_penicillin-vancomycin-cecal', 'cho_penicillin-vancomycin-fecal', 'turnbaugh_obese-lean-all'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "\n",
    "mlrepo_list = []\n",
    "\n",
    "# Hardcoded task mappings\n",
    "task_mapping = {\n",
    "    \"cho\": [\"control-ct-cecal\", \"control-ct-fecal\", \"penicillin-vancomycin-cecal\", \"penicillin-vancomycin-fecal\"],\n",
    "    \"gevers\": [\"ileum\", \"pcdai-ileum\", \"pcdai-rectum\", \"rectum\"],\n",
    "    \"hmp\": [\"gastro-oral\", \"sex\", \"stool-tongue-paired\", \"sub-supragingivalplaque-paired\"],\n",
    "    \"karlsson\": [\"impaired-diabetes\", \"normal-diabetes\"],\n",
    "    \"ravel\": [\"black-hispanic\", \"nugent-category\", \"nugent-score\", \"ph\", \"white-black\"],\n",
    "    \"sokol\": [\"healthy-cd\", \"healthy-uc\"],\n",
    "    \"turnbaugh\": [\"obese-lean-all\"],\n",
    "    \"yatsunenko\": [\"baby-age\", \"malawi-venezuela\", \"sex\", \"usa-malawi\"],\n",
    "}\n",
    "\n",
    "# Datasets to drop\n",
    "to_drop = [\"dethlefsen\", \"karlsson\", \"qin2012\", \"qin2014\", \"ridaura\"]\n",
    "\n",
    "mlrepo_dir = \"../data/otu_tables/mlrepo\"\n",
    "\n",
    "for subdir in os.listdir(mlrepo_dir):\n",
    "    if subdir in to_drop or not os.path.isdir(os.path.join(mlrepo_dir, subdir)):\n",
    "        continue\n",
    "\n",
    "    gg_path = os.path.join(mlrepo_dir, subdir, \"gg\", \"otutable.txt\")\n",
    "\n",
    "    if not os.path.exists(gg_path):\n",
    "        continue\n",
    "\n",
    "    gg_otu_table = pd.read_table(gg_path, index_col=0).T\n",
    "    # Convert index to string type to avoid ImplicitModificationWarning\n",
    "    gg_otu_table.index = gg_otu_table.index.astype(str)\n",
    "    task_adata = anndata.AnnData(gg_otu_table)\n",
    "\n",
    "    # Initialize an empty DataFrame for all labels\n",
    "    all_labels_df = pd.DataFrame(index=gg_otu_table.index)\n",
    "\n",
    "    task_names = task_mapping.get(subdir, [])\n",
    "\n",
    "    for task_name in task_names:\n",
    "        labels_path = os.path.join(mlrepo_dir, subdir, f\"task-{task_name}.txt\")\n",
    "\n",
    "        if os.path.exists(labels_path):\n",
    "            labels = pd.read_table(labels_path, index_col=0)\n",
    "            # Convert index to string type to avoid ImplicitModificationWarning\n",
    "            labels.index = labels.index.astype(str)\n",
    "            if \"ControlVar\" in labels.columns:\n",
    "                labels = labels.drop(columns=[\"ControlVar\"])\n",
    "\n",
    "            labels.columns = [f\"{subdir}_{task_name}\"]\n",
    "            all_labels_df = all_labels_df.join(labels, how=\"outer\")\n",
    "\n",
    "    # Ensure the final labels dataframe aligns with obs indices\n",
    "    task_adata.obs = task_adata.obs.join(all_labels_df, how=\"left\")\n",
    "    task_adata.obs[\"dataset\"] = subdir\n",
    "\n",
    "    mlrepo_list.append(task_adata)\n",
    "\n",
    "# Concatenate all AnnData objects with consistent vars\n",
    "mlrepo = anndata.concat(mlrepo_list, join=\"outer\", merge=\"same\")\n",
    "mlrepo.X = np.nan_to_num(mlrepo.X, nan=0)\n",
    "\n",
    "# Ensure all column names in obs are strings\n",
    "mlrepo.obs.columns = mlrepo.obs.columns.astype(str)\n",
    "\n",
    "print(mlrepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ravel_black-hispanic: {'Black': 104, 'Hispanic': 95}\n",
      "ravel_nugent-category: {'low': 245, 'high': 97}\n",
      "ravel_white-black: {'Black': 104, 'White': 96}\n",
      "gevers_ileum: {'CD': 78, 'no': 62}\n",
      "gevers_rectum: {'no': 92, 'CD': 68}\n",
      "sokol_healthy-cd: {\"Crohn's disease\": 59, 'Healthy': 15}\n",
      "sokol_healthy-uc: {'Ulcerative Colitis': 44, 'Healthy': 15}\n",
      "yatsunenko_malawi-venezuela: {'GAZ:Venezuela': 33, 'GAZ:Malawi': 21}\n",
      "yatsunenko_sex: {'female': 92, 'male': 37}\n",
      "yatsunenko_usa-malawi: {'GAZ:United States of America': 129, 'GAZ:Malawi': 21}\n",
      "hmp_gastro-oral: {'Oral': 1843, 'Gastrointestinal_tract': 227}\n",
      "hmp_sex: {'male': 98, 'female': 82}\n",
      "hmp_stool-tongue-paired: {'Stool': 204, 'Tongue_dorsum': 200}\n",
      "hmp_sub-supragingivalplaque-paired: {'Supragingival_plaque': 205, 'Subgingival_plaque': 203}\n",
      "cho_control-ct-cecal: {'Control': 10, 'Chlortetracycline': 7}\n",
      "cho_control-ct-fecal: {'Control': 10, 'Chlortetracycline': 8}\n",
      "cho_penicillin-vancomycin-cecal: {'Penicillin': 10, 'Vancomycin': 10}\n",
      "cho_penicillin-vancomycin-fecal: {'Vancomycin': 10, 'Penicillin': 9}\n",
      "turnbaugh_obese-lean-all: {'Obese': 107, 'Lean': 35}\n"
     ]
    }
   ],
   "source": [
    "# Verify that all metadata has some non-nan values\n",
    "mlrepo.obs.isna().all()\n",
    "\n",
    "# Print counts for each unique value\n",
    "for col in mlrepo.obs.columns:\n",
    "    # Skip dataset col\n",
    "    if col == \"dataset\":\n",
    "        continue\n",
    "\n",
    "    # Skip regression cols\n",
    "    if col in [\"ravel_nugent-score\", \"ravel_ph\", \"gevers_pcdai-ileum\", \"gevers_pcdai-rectum\", \"yatsunenko_baby-age\"]:\n",
    "        continue\n",
    "\n",
    "    print(f\"{col}: {mlrepo.obs[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to varm - need to drop first column and index with second column.\n",
    "# Reindexing with var_names is critical!\n",
    "import torch\n",
    "\n",
    "for geom in [\"H\", \"E\"]:\n",
    "    for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "        embeddings = pd.read_csv(f\"../data/otu_embeddings/greengenes/{geom}{dim}.csv.gz\", index_col=1)\n",
    "        embeddings = embeddings.drop(columns=[\"Unnamed: 0.1\"])\n",
    "        embeddings.index = [str(x) for x in embeddings.index]\n",
    "        mlrepo.varm[f\"{geom}{dim}\"] = embeddings.reindex(mlrepo.var_names).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2\n",
      "H4\n",
      "H8\n",
      "H16\n",
      "H32\n",
      "H64\n",
      "H128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc114f8412354ebe87e3c7018dbd10a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate mixture embeddings\n",
    "import geoopt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "MAN = geoopt.manifolds.PoincareBall().to(DEVICE)\n",
    "\n",
    "# Euclidean case\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    mlrepo.obsm[f\"E{dim}\"] = mlrepo.X @ mlrepo.varm[f\"E{dim}\"]  # (n_obs, n_vars) @ (n_vars, dim) -> (n_obs, dim)\n",
    "\n",
    "# Hyperbolic case\n",
    "abundances_tensor = torch.tensor(mlrepo.X, device=DEVICE)\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    print(f\"H{dim}\")\n",
    "\n",
    "    # Need this workaround to not crash the kernel - it's just the iterative version of the midpoint operation\n",
    "    # (see following cell for proof of concept with H2)\n",
    "    embeddings_tensor = torch.tensor(mlrepo.varm[f\"H{dim}\"], device=DEVICE).unsqueeze(0)\n",
    "    if dim == 128:\n",
    "        out = [MAN.weighted_midpoint(embeddings_tensor, row).numpy() for row in tqdm(abundances_tensor)]\n",
    "        out = np.array(out)\n",
    "\n",
    "    else:\n",
    "        out = (\n",
    "            MAN.weighted_midpoint(xs=embeddings_tensor, weights=abundances_tensor, reducedim=[1]).detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "    # Some validation here\n",
    "    assert out.shape == (mlrepo.n_obs, dim)\n",
    "    assert not np.isnan(out).any()\n",
    "    # assert MAN.assert_check_point_on_manifold(out) # Commented out because making the types work is a pain\n",
    "    # I do it in a subsequent cell instead\n",
    "    mlrepo.obsm[f\"H{dim}\"] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10037, 2)\n",
      "(10037, 2)\n"
     ]
    }
   ],
   "source": [
    "# Proof of concept: our vectorization works\n",
    "\n",
    "# Get manifold and other shared objects\n",
    "p2 = geoopt.manifolds.PoincareBall(2)\n",
    "h2_tensor = torch.tensor(mlrepo.varm[\"H2\"]).unsqueeze(0)\n",
    "X_tensor = torch.tensor(mlrepo.X)\n",
    "\n",
    "# Iterative version we use for H128\n",
    "out = []\n",
    "for row in X_tensor:\n",
    "    out.append(p2.weighted_midpoint(h2_tensor, row).numpy())\n",
    "out = np.array(out)\n",
    "print(out.shape)\n",
    "assert not np.isnan(out).any()\n",
    "\n",
    "# Now do the vectorized version\n",
    "out_vectorized = p2.weighted_midpoint(h2_tensor, X_tensor, reducedim=[1]).detach().cpu().numpy()\n",
    "print(out_vectorized.shape)\n",
    "assert not np.isnan(out_vectorized).any()\n",
    "\n",
    "# They match!\n",
    "assert np.allclose(out, out_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all points are on the manifold\n",
    "for k, v in mlrepo.varm.items():\n",
    "    if k.startswith(\"H\"):\n",
    "        assert MAN.check_point_on_manifold(torch.tensor(v)), f\"{k} is not on the manifold\"\n",
    "\n",
    "for k, v in mlrepo.obsm.items():\n",
    "    if k.startswith(\"H\"):\n",
    "        assert MAN.check_point_on_manifold(torch.tensor(v)), f\"{k} is not on the manifold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_425661/2374699152.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  gg_tax = pd.read_csv(gg_tax_path, index_col=0, sep=\"\\t|;\", header=None)\n"
     ]
    }
   ],
   "source": [
    "# Add greengenes taxonomic information\n",
    "\n",
    "gg_tax_path = \"../data/greengenes/gg_13_5_taxonomy.txt.gz\"\n",
    "\n",
    "# Read taxonomy file - separators are tab AND semicolon\n",
    "gg_tax = pd.read_csv(gg_tax_path, index_col=0, sep=\"\\t|;\", header=None)\n",
    "gg_tax.index = [str(x) for x in gg_tax.index]\n",
    "gg_tax.columns = [\"k\", \"p\", \"c\", \"o\", \"f\", \"g\", \"s\"]\n",
    "for col in gg_tax.columns:\n",
    "    # Strip whitespace and remove the prefix pattern (e.g., \"k__\", \"p__\")\n",
    "    gg_tax[col] = gg_tax[col].str.strip().str.replace(f\"{col}__\", \"\", regex=False)\n",
    "\n",
    "# Add to mlrepo.var\n",
    "mlrepo.var = mlrepo.var.join(gg_tax, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 10037 × 27105\n",
       "    obs: 'ravel_black-hispanic', 'ravel_nugent-category', 'ravel_nugent-score', 'ravel_ph', 'ravel_white-black', 'dataset', 'gevers_ileum', 'gevers_pcdai-ileum', 'gevers_pcdai-rectum', 'gevers_rectum', 'sokol_healthy-cd', 'sokol_healthy-uc', 'yatsunenko_baby-age', 'yatsunenko_malawi-venezuela', 'yatsunenko_sex', 'yatsunenko_usa-malawi', 'hmp_gastro-oral', 'hmp_sex', 'hmp_stool-tongue-paired', 'hmp_sub-supragingivalplaque-paired', 'cho_control-ct-cecal', 'cho_control-ct-fecal', 'cho_penicillin-vancomycin-cecal', 'cho_penicillin-vancomycin-fecal', 'turnbaugh_obese-lean-all'\n",
       "    var: 'k', 'p', 'c', 'o', 'f', 'g', 's'\n",
       "    obsm: 'E2', 'E4', 'E8', 'E16', 'E32', 'E64', 'E128', 'H2', 'H4', 'H8', 'H16', 'H32', 'H64', 'H128'\n",
       "    varm: 'H2', 'H4', 'H8', 'H16', 'H32', 'H64', 'H128', 'E2', 'E4', 'E8', 'E16', 'E32', 'E64', 'E128'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final look at mlrepo\n",
    "mlrepo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all column names are strings\n",
    "mlrepo.var.columns = [str(x) for x in mlrepo.var.columns]\n",
    "mlrepo.obs.columns = [str(x) for x in mlrepo.obs.columns]\n",
    "\n",
    "# Save mlrepo\n",
    "mlrepo.write_h5ad(\"../data/mlrepo.h5ad.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
