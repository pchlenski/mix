{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data\n",
    "> All of the data processing code I use for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroSEED embeddings - hyperbolic/Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first: gzip all the embeddings so we don't run into LFS limits\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "embed_dir = \"../data/otu_embeddings/greengenes\"\n",
    "for file in os.listdir(embed_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        old_path = os.path.join(embed_dir, file)\n",
    "        new_path = os.path.join(\n",
    "            embed_dir,\n",
    "            file.replace(\"embeddings_\", \"\").replace(\"hyperbolic\", \"H\").replace(\"euclidean\", \"E\").replace(\"_\", \"\")\n",
    "            + \".gz\",\n",
    "        )  # \"embeddings_\" prefix redundant\n",
    "        print(f\"Compressing {file}\")\n",
    "        pd.read_csv(old_path).to_csv(new_path, compression=\"gzip\")\n",
    "        print(f\"Compressed {file} from {os.path.getsize(old_path)} to {os.path.getsize(new_path)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! Now we can delete the old embeddings to clear some space\n",
    "for file in os.listdir(embed_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(embed_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added these datasets to huggingface:\n",
    "https://huggingface.co/datasets/pchlenski/greengenes_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRepo embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(gg_path):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m gg_otu_table \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Convert index to string type to avoid ImplicitModificationWarning\u001b[39;00m\n\u001b[1;32m     36\u001b[0m gg_otu_table\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m gg_otu_table\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1405\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1392\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1393\u001b[0m     dialect,\n\u001b[1;32m   1394\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1402\u001b[0m )\n\u001b[1;32m   1403\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "\n",
    "mlrepo_list = []\n",
    "\n",
    "# Hardcoded task mappings\n",
    "task_mapping = {\n",
    "    \"cho\": [\"control-ct-cecal\", \"control-ct-fecal\", \"penicillin-vancomycin-cecal\", \"penicillin-vancomycin-fecal\"],\n",
    "    \"gevers\": [\"ileum\", \"pcdai-ileum\", \"pcdai-rectum\", \"rectum\"],\n",
    "    \"hmp\": [\"gastro-oral\", \"sex\", \"stool-tongue-paired\", \"sub-supragingivalplaque-paired\"],\n",
    "    \"karlsson\": [\"impaired-diabetes\", \"normal-diabetes\"],\n",
    "    \"ravel\": [\"black-hispanic\", \"nugent-category\", \"nugent-score\", \"ph\", \"white-black\"],\n",
    "    \"sokol\": [\"healthy-cd\", \"healthy-uc\"],\n",
    "    \"turnbaugh\": [\"obese-lean-all\"],\n",
    "    \"yatsunenko\": [\"baby-age\", \"malawi-venezuela\", \"sex\", \"usa-malawi\"],\n",
    "}\n",
    "\n",
    "# Datasets to drop\n",
    "to_drop = [\"dethlefsen\", \"karlsson\", \"qin2012\", \"qin2014\", \"ridaura\"]\n",
    "\n",
    "mlrepo_dir = \"../data/otu_tables/mlrepo\"\n",
    "\n",
    "for subdir in os.listdir(mlrepo_dir):\n",
    "    if subdir in to_drop or not os.path.isdir(os.path.join(mlrepo_dir, subdir)):\n",
    "        continue\n",
    "\n",
    "    gg_path = os.path.join(mlrepo_dir, subdir, \"gg\", \"otutable.txt\")\n",
    "\n",
    "    if not os.path.exists(gg_path):\n",
    "        continue\n",
    "\n",
    "    gg_otu_table = pd.read_table(gg_path, index_col=0).T\n",
    "    # Convert index to string type to avoid ImplicitModificationWarning\n",
    "    gg_otu_table.index = gg_otu_table.index.astype(str)\n",
    "    task_adata = anndata.AnnData(gg_otu_table)\n",
    "\n",
    "    # Initialize an empty DataFrame for all labels\n",
    "    all_labels_df = pd.DataFrame(index=gg_otu_table.index)\n",
    "\n",
    "    task_names = task_mapping.get(subdir, [])\n",
    "\n",
    "    for task_name in task_names:\n",
    "        labels_path = os.path.join(mlrepo_dir, subdir, f\"task-{task_name}.txt\")\n",
    "\n",
    "        if os.path.exists(labels_path):\n",
    "            labels = pd.read_table(labels_path, index_col=0)\n",
    "            # Convert index to string type to avoid ImplicitModificationWarning\n",
    "            labels.index = labels.index.astype(str)\n",
    "            if \"ControlVar\" in labels.columns:\n",
    "                labels = labels.drop(columns=[\"ControlVar\"])\n",
    "\n",
    "            labels.columns = [f\"{subdir}_{task_name}\"]\n",
    "            all_labels_df = all_labels_df.join(labels, how=\"outer\")\n",
    "\n",
    "    # Ensure the final labels dataframe aligns with obs indices\n",
    "    task_adata.obs = task_adata.obs.join(all_labels_df, how=\"left\")\n",
    "    task_adata.obs[\"dataset\"] = subdir\n",
    "\n",
    "    mlrepo_list.append(task_adata)\n",
    "\n",
    "# Concatenate all AnnData objects with consistent vars\n",
    "mlrepo = anndata.concat(mlrepo_list, join=\"outer\", merge=\"same\")\n",
    "mlrepo.X = np.nan_to_num(mlrepo.X, nan=0)\n",
    "\n",
    "# Ensure all column names in obs are strings\n",
    "mlrepo.obs.columns = mlrepo.obs.columns.astype(str)\n",
    "\n",
    "print(mlrepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ravel_black-hispanic: {'Black': 104, 'Hispanic': 95}\n",
      "ravel_nugent-category: {'low': 245, 'high': 97}\n",
      "ravel_white-black: {'Black': 104, 'White': 96}\n",
      "gevers_ileum: {'CD': 78, 'no': 62}\n",
      "gevers_rectum: {'no': 92, 'CD': 68}\n",
      "sokol_healthy-cd: {\"Crohn's disease\": 59, 'Healthy': 15}\n",
      "sokol_healthy-uc: {'Ulcerative Colitis': 44, 'Healthy': 15}\n",
      "yatsunenko_malawi-venezuela: {'GAZ:Venezuela': 33, 'GAZ:Malawi': 21}\n",
      "yatsunenko_sex: {'female': 92, 'male': 37}\n",
      "yatsunenko_usa-malawi: {'GAZ:United States of America': 129, 'GAZ:Malawi': 21}\n",
      "hmp_gastro-oral: {'Oral': 1843, 'Gastrointestinal_tract': 227}\n",
      "hmp_sex: {'male': 98, 'female': 82}\n",
      "hmp_stool-tongue-paired: {'Stool': 204, 'Tongue_dorsum': 200}\n",
      "hmp_sub-supragingivalplaque-paired: {'Supragingival_plaque': 205, 'Subgingival_plaque': 203}\n",
      "cho_control-ct-cecal: {'Control': 10, 'Chlortetracycline': 7}\n",
      "cho_control-ct-fecal: {'Control': 10, 'Chlortetracycline': 8}\n",
      "cho_penicillin-vancomycin-cecal: {'Penicillin': 10, 'Vancomycin': 10}\n",
      "cho_penicillin-vancomycin-fecal: {'Vancomycin': 10, 'Penicillin': 9}\n",
      "turnbaugh_obese-lean-all: {'Obese': 107, 'Lean': 35}\n"
     ]
    }
   ],
   "source": [
    "# Verify that all metadata has some non-nan values\n",
    "mlrepo.obs.isna().all()\n",
    "\n",
    "# Print counts for each unique value\n",
    "for col in mlrepo.obs.columns:\n",
    "    # Skip dataset col\n",
    "    if col == \"dataset\":\n",
    "        continue\n",
    "\n",
    "    # Skip regression cols\n",
    "    if col in [\"ravel_nugent-score\", \"ravel_ph\", \"gevers_pcdai-ileum\", \"gevers_pcdai-rectum\", \"yatsunenko_baby-age\"]:\n",
    "        continue\n",
    "\n",
    "    print(f\"{col}: {mlrepo.obs[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to varm - need to drop first column and index with second column.\n",
    "# Reindexing with var_names is critical!\n",
    "import torch\n",
    "\n",
    "for geom in [\"H\", \"E\"]:\n",
    "    for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "        embeddings = pd.read_csv(f\"../data/otu_embeddings/greengenes/{geom}{dim}.csv.gz\", index_col=1)\n",
    "        embeddings = embeddings.drop(columns=[\"Unnamed: 0.1\"])\n",
    "        embeddings.index = [str(x) for x in embeddings.index]\n",
    "        mlrepo.varm[f\"{geom}{dim}\"] = embeddings.reindex(mlrepo.var_names).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2\n",
      "H4\n",
      "H8\n",
      "H16\n",
      "H32\n",
      "H64\n",
      "H128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc114f8412354ebe87e3c7018dbd10a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate mixture embeddings\n",
    "import geoopt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "MAN = geoopt.manifolds.PoincareBall().to(DEVICE)\n",
    "\n",
    "# Euclidean case\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    mlrepo.obsm[f\"E{dim}\"] = mlrepo.X @ mlrepo.varm[f\"E{dim}\"]  # (n_obs, n_vars) @ (n_vars, dim) -> (n_obs, dim)\n",
    "\n",
    "# Hyperbolic case\n",
    "abundances_tensor = torch.tensor(mlrepo.X, device=DEVICE)\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    print(f\"H{dim}\")\n",
    "\n",
    "    # Need this workaround to not crash the kernel - it's just the iterative version of the midpoint operation\n",
    "    # (see following cell for proof of concept with H2)\n",
    "    embeddings_tensor = torch.tensor(mlrepo.varm[f\"H{dim}\"], device=DEVICE).unsqueeze(0)\n",
    "    if dim == 128:\n",
    "        out = [MAN.weighted_midpoint(embeddings_tensor, row).numpy() for row in tqdm(abundances_tensor)]\n",
    "        out = np.array(out)\n",
    "\n",
    "    else:\n",
    "        out = (\n",
    "            MAN.weighted_midpoint(xs=embeddings_tensor, weights=abundances_tensor, reducedim=[1]).detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "    # Some validation here\n",
    "    assert out.shape == (mlrepo.n_obs, dim)\n",
    "    assert not np.isnan(out).any()\n",
    "    # assert MAN.assert_check_point_on_manifold(out) # Commented out because making the types work is a pain\n",
    "    # I do it in a subsequent cell instead\n",
    "    mlrepo.obsm[f\"H{dim}\"] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10037, 2)\n",
      "(10037, 2)\n"
     ]
    }
   ],
   "source": [
    "# Proof of concept: our vectorization works\n",
    "\n",
    "# Get manifold and other shared objects\n",
    "p2 = geoopt.manifolds.PoincareBall(2)\n",
    "h2_tensor = torch.tensor(mlrepo.varm[\"H2\"]).unsqueeze(0)\n",
    "X_tensor = torch.tensor(mlrepo.X)\n",
    "\n",
    "# Iterative version we use for H128\n",
    "out = []\n",
    "for row in X_tensor:\n",
    "    out.append(p2.weighted_midpoint(h2_tensor, row).numpy())\n",
    "out = np.array(out)\n",
    "print(out.shape)\n",
    "assert not np.isnan(out).any()\n",
    "\n",
    "# Now do the vectorized version\n",
    "out_vectorized = p2.weighted_midpoint(h2_tensor, X_tensor, reducedim=[1]).detach().cpu().numpy()\n",
    "print(out_vectorized.shape)\n",
    "assert not np.isnan(out_vectorized).any()\n",
    "\n",
    "# They match!\n",
    "assert np.allclose(out, out_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all points are on the manifold\n",
    "for k, v in mlrepo.varm.items():\n",
    "    if k.startswith(\"H\"):\n",
    "        assert MAN.check_point_on_manifold(torch.tensor(v)), f\"{k} is not on the manifold\"\n",
    "\n",
    "for k, v in mlrepo.obsm.items():\n",
    "    if k.startswith(\"H\"):\n",
    "        assert MAN.check_point_on_manifold(torch.tensor(v)), f\"{k} is not on the manifold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_425661/2374699152.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  gg_tax = pd.read_csv(gg_tax_path, index_col=0, sep=\"\\t|;\", header=None)\n"
     ]
    }
   ],
   "source": [
    "# Add greengenes taxonomic information\n",
    "\n",
    "gg_tax_path = \"../data/greengenes/gg_13_5_taxonomy.txt.gz\"\n",
    "\n",
    "# Read taxonomy file - separators are tab AND semicolon\n",
    "gg_tax = pd.read_csv(gg_tax_path, index_col=0, sep=\"\\t|;\", header=None)\n",
    "gg_tax.index = [str(x) for x in gg_tax.index]\n",
    "gg_tax.columns = [\"k\", \"p\", \"c\", \"o\", \"f\", \"g\", \"s\"]\n",
    "for col in gg_tax.columns:\n",
    "    # Strip whitespace and remove the prefix pattern (e.g., \"k__\", \"p__\")\n",
    "    gg_tax[col] = gg_tax[col].str.strip().str.replace(f\"{col}__\", \"\", regex=False)\n",
    "\n",
    "# Add to mlrepo.var\n",
    "mlrepo.var = mlrepo.var.join(gg_tax, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNABERT-S embeddings and FASTA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (1.85)\n",
      "Requirement already satisfied: numpy in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from biopython) (1.26.4)\n",
      "Requirement already satisfied: transformers in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: einops in /home/phil/miniconda3/envs/embedders/lib/python3.9/site-packages (0.8.1)\n",
      "\u001b[33mWARNING: Skipping triton as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install biopython\n",
    "!pip install transformers\n",
    "!pip install einops\n",
    "!pip uninstall triton -y # triton breaks dnabert-s, idk why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLRepo anndata from checkpoint\n",
    "mlrepo = anndata.read_h5ad(\"../data/mlrepo.h5ad.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FASTA sequences to mlrepo.var\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Get FASTA sequences by Greengenes ID\n",
    "gg_fasta_path = \"../data/greengenes/gg_13_5.fasta.gz\"\n",
    "\n",
    "# Read FASTA file - Pearson format to avoid warnings due to comments in the Greengenes file\n",
    "gg_fasta = {rec.id: str(rec.seq) for rec in SeqIO.parse(gzip.open(gg_fasta_path, \"rt\"), format=\"fasta\")}\n",
    "\n",
    "# Add to mlrepo.var\n",
    "assert not (set(mlrepo.var.index) - set(gg_fasta.keys()))  # mlrepo index subset of gg_fasta keys\n",
    "mlrepo.var[\"fasta\"] = mlrepo.var.index.map(gg_fasta)\n",
    "\n",
    "del gg_fasta  # This is a big dict, so we'll delete it to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-S/2efd650282ec5d5ab377c787c76ea56b723f6b7c/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d68cea34614d7f825b90d78ddc5f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27105, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-S\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-S\", trust_remote_code=True)\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "# Make embeddings for each fasta\n",
    "def compute_dnabert_embedding(seq):\n",
    "    \"\"\"Mean-pooled embedding of the entire sequence - based on https://github.com/MAGICS-LAB/DNABERT_S README.md\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(seq, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
    "        hidden_states = model(inputs)[0]  # (1, seq_length, 768)\n",
    "        embedding_mean = hidden_states[0].mean(dim=0)  # (768,)\n",
    "    return embedding_mean.cpu().numpy()\n",
    "\n",
    "\n",
    "mlrepo.varm[\"dnabert-s\"] = np.array([compute_dnabert_embedding(seq) for seq in tqdm(mlrepo.var[\"fasta\"])])\n",
    "print(mlrepo.varm[\"dnabert-s\"].shape)\n",
    "assert not np.isnan(mlrepo.varm[\"dnabert-s\"]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mixture embeddings - same as Euclidean case\n",
    "\n",
    "mlrepo.obsm[\"dnabert-s\"] = mlrepo.X @ mlrepo.varm[\"dnabert-s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 10037 × 27105\n",
       "    obs: 'ravel_black-hispanic', 'ravel_nugent-category', 'ravel_nugent-score', 'ravel_ph', 'ravel_white-black', 'dataset', 'gevers_ileum', 'gevers_pcdai-ileum', 'gevers_pcdai-rectum', 'gevers_rectum', 'sokol_healthy-cd', 'sokol_healthy-uc', 'yatsunenko_baby-age', 'yatsunenko_malawi-venezuela', 'yatsunenko_sex', 'yatsunenko_usa-malawi', 'hmp_gastro-oral', 'hmp_sex', 'hmp_stool-tongue-paired', 'hmp_sub-supragingivalplaque-paired', 'cho_control-ct-cecal', 'cho_control-ct-fecal', 'cho_penicillin-vancomycin-cecal', 'cho_penicillin-vancomycin-fecal', 'turnbaugh_obese-lean-all'\n",
       "    var: 'k', 'p', 'c', 'o', 'f', 'g', 's', 'fasta'\n",
       "    obsm: 'E128', 'E16', 'E2', 'E32', 'E4', 'E64', 'E8', 'H128', 'H16', 'H2', 'H32', 'H4', 'H64', 'H8', 'dnabert-s'\n",
       "    varm: 'E128', 'E16', 'E2', 'E32', 'E4', 'E64', 'E8', 'H128', 'H16', 'H2', 'H32', 'H4', 'H64', 'H8', 'dnabert-s'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final look at mlrepo\n",
    "mlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all column names are strings\n",
    "mlrepo.var.columns = [str(x) for x in mlrepo.var.columns]\n",
    "mlrepo.obs.columns = [str(x) for x in mlrepo.obs.columns]\n",
    "\n",
    "# Save mlrepo\n",
    "mlrepo.write_h5ad(\"../data/mlrepo.h5ad.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
